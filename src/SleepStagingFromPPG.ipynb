{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-7dc49faa3c0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mgpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_physical_devices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GPU'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_memory_growth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_logger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetLevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'INFO'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='1' #Set the GPU you wish to use here\n",
    "\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "display(gpus)\n",
    "tf.config.experimental.set_memory_growth(gpus[0], enable=True)\n",
    "tf.get_logger().setLevel('INFO')\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from src.models.sleepstagecnn import *\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "# logging.getLogger('tensorflow').setLevel(logging.FATAL)\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.python.keras import backend as K\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from skopt.utils import use_named_args\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from termcolor import colored\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from src.datasets.sleep_staging_dataset import SleepStagingDataset\n",
    "import utils.paths as paths\n",
    "from utils.consts import *\n",
    "from utils.sleep_extract import *\n",
    "from utils.utils import *\n",
    "\n",
    "def time_series_subsequences(ts, window, hop=1):\n",
    "    window = int(window)\n",
    "    hop = int(hop)\n",
    "    assert len(ts.shape) == 1\n",
    "    shape = (int(int(ts.size - window) / hop + 1), window)\n",
    "    strides = ts.strides[0] * hop, ts.strides[0]\n",
    "    return np.lib.stride_tricks.as_strided(ts, shape=shape, strides=strides)\n",
    "\n",
    "def signal_quality(patient_id):\n",
    "    quality_dir = data_dir / 'quality' / 'ppg' / f'mesa-sleep-{patient_id}-Pleth-Quality.xlsx'\n",
    "    quality = pd.read_excel(quality_dir)\n",
    "    return quality\n",
    "    \n",
    "def ppg_data(patient_id):\n",
    "    ppg = np.load(data_dir / 'npys' / f'mesa-sleep-{patient_id}-Pleth.npy')\n",
    "    return ppg\n",
    "\n",
    "def sleep_data(patient_id):\n",
    "    sleep = sleep_extract_30s_epochs(data_dir / 'annotations-events-nsrr' / f'mesa-sleep-{patient_id}-nsrr.xml')\n",
    "    return sleep\n",
    "\n",
    "def intersection(lst1, lst2): \n",
    "    return list(set(lst1) & set(lst2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-gpu in /home/daniel/anaconda3/envs/cs236781-proj-keras-ver3/lib/python3.8/site-packages (2.3.1)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /home/daniel/anaconda3/envs/cs236781-proj-keras-ver3/lib/python3.8/site-packages (from tensorflow-gpu) (1.12.1)\n",
      "Requirement already satisfied: astunparse==1.6.3 in /home/daniel/anaconda3/envs/cs236781-proj-keras-ver3/lib/python3.8/site-packages (from tensorflow-gpu) (1.6.3)\n",
      "Requirement already satisfied: wheel>=0.26 in /home/daniel/anaconda3/envs/cs236781-proj-keras-ver3/lib/python3.8/site-packages (from tensorflow-gpu) (0.35.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /home/daniel/anaconda3/envs/cs236781-proj-keras-ver3/lib/python3.8/site-packages (from tensorflow-gpu) (2.3.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/daniel/anaconda3/envs/cs236781-proj-keras-ver3/lib/python3.8/site-packages (from tensorflow-gpu) (1.1.0)\n",
      "Requirement already satisfied: gast==0.3.3 in /home/daniel/anaconda3/envs/cs236781-proj-keras-ver3/lib/python3.8/site-packages (from tensorflow-gpu) (0.3.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/daniel/anaconda3/envs/cs236781-proj-keras-ver3/lib/python3.8/site-packages (from tensorflow-gpu) (3.1.0)\n",
      "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /home/daniel/anaconda3/envs/cs236781-proj-keras-ver3/lib/python3.8/site-packages (from tensorflow-gpu) (1.1.2)\n",
      "Requirement already satisfied: tensorboard<3,>=2.3.0 in /home/daniel/anaconda3/envs/cs236781-proj-keras-ver3/lib/python3.8/site-packages (from tensorflow-gpu) (2.3.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/daniel/anaconda3/envs/cs236781-proj-keras-ver3/lib/python3.8/site-packages (from tensorflow-gpu) (1.15.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /home/daniel/anaconda3/envs/cs236781-proj-keras-ver3/lib/python3.8/site-packages (from tensorflow-gpu) (1.31.0)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /home/daniel/anaconda3/envs/cs236781-proj-keras-ver3/lib/python3.8/site-packages (from tensorflow-gpu) (2.10.0)\n",
      "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /home/daniel/anaconda3/envs/cs236781-proj-keras-ver3/lib/python3.8/site-packages (from tensorflow-gpu) (1.18.5)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /home/daniel/anaconda3/envs/cs236781-proj-keras-ver3/lib/python3.8/site-packages (from tensorflow-gpu) (0.10.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /home/daniel/anaconda3/envs/cs236781-proj-keras-ver3/lib/python3.8/site-packages (from tensorflow-gpu) (3.13.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in /home/daniel/anaconda3/envs/cs236781-proj-keras-ver3/lib/python3.8/site-packages (from tensorflow-gpu) (0.2.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/daniel/anaconda3/envs/cs236781-proj-keras-ver3/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu) (2.24.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /home/daniel/anaconda3/envs/cs236781-proj-keras-ver3/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu) (1.22.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/daniel/anaconda3/envs/cs236781-proj-keras-ver3/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu) (1.0.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/daniel/anaconda3/envs/cs236781-proj-keras-ver3/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu) (0.4.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/daniel/anaconda3/envs/cs236781-proj-keras-ver3/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu) (3.3)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/daniel/anaconda3/envs/cs236781-proj-keras-ver3/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu) (1.6.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/daniel/anaconda3/envs/cs236781-proj-keras-ver3/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu) (50.3.0.post20201006)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/daniel/anaconda3/envs/cs236781-proj-keras-ver3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow-gpu) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/daniel/anaconda3/envs/cs236781-proj-keras-ver3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow-gpu) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/daniel/anaconda3/envs/cs236781-proj-keras-ver3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow-gpu) (1.25.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/daniel/anaconda3/envs/cs236781-proj-keras-ver3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow-gpu) (3.0.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.5\" in /home/daniel/anaconda3/envs/cs236781-proj-keras-ver3/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-gpu) (4.6)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/daniel/anaconda3/envs/cs236781-proj-keras-ver3/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-gpu) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/daniel/anaconda3/envs/cs236781-proj-keras-ver3/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-gpu) (4.1.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/daniel/anaconda3/envs/cs236781-proj-keras-ver3/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow-gpu) (1.3.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /home/daniel/anaconda3/envs/cs236781-proj-keras-ver3/lib/python3.8/site-packages (from rsa<5,>=3.1.4; python_version >= \"3.5\"->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-gpu) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/daniel/anaconda3/envs/cs236781-proj-keras-ver3/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow-gpu) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(these_patient_IDs, min_quality):\n",
    "    \n",
    "    ppg = {}\n",
    "    sleep = {}\n",
    "    quality = {}\n",
    "    quality_mean = {}\n",
    "    could_not_load = []\n",
    "    \n",
    "    # Load and downsample data and fetch the patient quality\n",
    "    for patient in tqdm.tqdm(these_patient_IDs):\n",
    "        try:\n",
    "            quality[patient] = signal_quality(patient).quality.to_numpy()\n",
    "            quality_mean[patient] = quality[patient][10:int(len(quality[patient])*0.95)].mean()\n",
    "            if quality_mean[patient] < min_quality:\n",
    "                raise ValueError('Does not meet quality requirements')\n",
    "            ppg[patient] = filter_and_downsample(ppg_data(patient), in_fs=fs_PPG, out_fs=new_fs_PPG)\n",
    "            sleep[patient] = sleep_data(patient)\n",
    "        except: \n",
    "            could_not_load.append(patient)\n",
    "            try: del ppg[patient]\n",
    "            except: pass\n",
    "            try: del sleep[patient]\n",
    "            except: pass\n",
    "            try: del quality[patient]\n",
    "            except: pass\n",
    "            try: del quality_mean[patient]\n",
    "            except: pass\n",
    "\n",
    "    if len(could_not_load) > 0:\n",
    "        print('Could not load: ', len(could_not_load), 'of ', len(these_patient_IDs), \"patients.\", flush=True)\n",
    "    these_patient_IDs = [patient for patient in these_patient_IDs if patient not in could_not_load]\n",
    "    \n",
    "    return ppg, sleep, quality, quality_mean\n",
    "\n",
    "def subsequences(ppg, sleep, quality):\n",
    "    ppg_subs = {}\n",
    "    sleep_subs = {}\n",
    "    quality_subs = {}\n",
    "\n",
    "    for patient in tqdm.tqdm(ppg.keys()):    \n",
    "        quality_subs[patient] = time_series_subsequences(quality[patient], dT * fs_sleep * windows, hop = dT * fs_sleep * hops)\n",
    "        sleep_subs[patient] = time_series_subsequences(sleep[patient], dT * fs_sleep * windows, hop = dT * fs_sleep * hops)[0:quality_subs[patient].shape[0], :]\n",
    "        ppg_subs[patient] = time_series_subsequences(ppg[patient], dT * new_fs_PPG * windows, hop = dT * new_fs_PPG * hops)[0:quality_subs[patient].shape[0], :]\n",
    "        \n",
    "        \n",
    "    return ppg_subs, sleep_subs, quality_subs\n",
    "\n",
    "\n",
    "def all_patients_subs_dataframe(ppg_subs, sleep_subs, quality_subs):\n",
    "\n",
    "    all_patient_subs = pd.DataFrame()\n",
    "    for patient in ppg_subs.keys():\n",
    "        this_patient_subs = pd.DataFrame({'patient': [patient]*sleep_subs[patient].shape[0], \n",
    "                                          'sleep': sleep_subs[patient][:,2], \n",
    "                                          'quality': quality_subs[patient].min(axis=1), \n",
    "                                          'epoch': np.arange(0, sleep_subs[patient].shape[0])})\n",
    "        all_patient_subs = pd.concat([all_patient_subs, this_patient_subs])\n",
    "    return all_patient_subs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mRunning in Quick Test Mode with 50.0% of data\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 20/698 [00:09<05:34,  2.03it/s]"
     ]
    }
   ],
   "source": [
    "#1) Load all data into memory\n",
    "#2) Create subsequences of the chosen duration/windows\n",
    "#3) Train Validate split on the patient_subs dataframe\n",
    "#4) Train a model on the data.....save all the result for the split\n",
    "\n",
    "name = 'Sleep_Alternative'\n",
    "\n",
    "# Quicktest\n",
    "QUICK_TEST_RATIO = 0.01 #1/3 #Set to 1 for the full dataset\n",
    "if QUICK_TEST_RATIO < 1: print(colored(f'Running in Quick Test Mode with {QUICK_TEST_RATIO*100}% of data', 'yellow'), flush=True)\n",
    "\n",
    "# Paths are set in utils.paths\n",
    "data_dir = paths.data_dir\n",
    "saved_dataset_dir = paths.saved_dataset_dir\n",
    "\n",
    "# Patient IDs\n",
    "patient_IDs = np.array([file[11:15] for file in os.listdir(data_dir / 'npys')])\n",
    "patient_IDs = np.unique(np.array([f\"{patient_ID}\".zfill(4) for patient_ID in patient_IDs]))\n",
    "\n",
    "train_patient_IDs, test_patient_IDs = train_test_split(patient_IDs, test_size=0.15, random_state=321)\n",
    "train_patient_IDs, validation_patient_IDs = train_test_split(train_patient_IDs, test_size=0.2, random_state=222)\n",
    "\n",
    "train_patient_IDs = train_patient_IDs[0:int(len(train_patient_IDs)*QUICK_TEST_RATIO)]\n",
    "validation_patient_IDs = validation_patient_IDs[0:int(len(validation_patient_IDs)*QUICK_TEST_RATIO)]\n",
    "test_patient_IDs = test_patient_IDs[0:int(len(test_patient_IDs)*QUICK_TEST_RATIO)]\n",
    "\n",
    "train_ppg, train_sleep, train_quality, train_quality_mean = load_data(train_patient_IDs, min_quality = 0.5)\n",
    "validation_ppg, validation_sleep, validation_quality, validation_quality_mean = load_data(validation_patient_IDs, min_quality=0.6)\n",
    "test_ppg, test_sleep, test_quality, test_quality_mean = load_data(test_patient_IDs, min_quality=0.6)\n",
    "\n",
    "train_ppg_subs, train_sleep_subs, train_quality_subs = subsequences(train_ppg, train_sleep, train_quality)\n",
    "validation_ppg_subs, validation_sleep_subs, validation_quality_subs = subsequences(validation_ppg, validation_sleep, validation_quality)\n",
    "test_ppg_subs, test_sleep_subs, test_quality_subs = subsequences(test_ppg, test_sleep, test_quality)\n",
    "\n",
    "train_dataframe = all_patients_subs_dataframe(train_ppg_subs, train_sleep_subs, train_quality_subs)\n",
    "validation_dataframe = all_patients_subs_dataframe(validation_ppg_subs, validation_sleep_subs, validation_quality_subs)\n",
    "test_dataframe = all_patients_subs_dataframe(test_ppg_subs, test_sleep_subs, test_quality_subs)\n",
    "\n",
    "train_dataframe = train_dataframe[train_dataframe.sleep < 9]\n",
    "validation_dataframe = validation_dataframe[validation_dataframe.sleep < 9]\n",
    "test_dataframe = test_dataframe[test_dataframe.sleep < 9]\n",
    "\n",
    "train_dataframe = train_dataframe[train_dataframe.quality > 0.6]\n",
    "validation_dataframe = validation_dataframe[validation_dataframe.quality > 0.6]\n",
    "test_dataframe = test_dataframe[test_dataframe.quality > 0.6]\n",
    "\n",
    "train_dataframe = train_dataframe.replace({'sleep': sleep_encoding})\n",
    "validation_dataframe = validation_dataframe.replace({'sleep': sleep_encoding})\n",
    "test_dataframe = test_dataframe.replace({'sleep': sleep_encoding})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient</th>\n",
       "      <th>sleep</th>\n",
       "      <th>quality</th>\n",
       "      <th>epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>2896</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>633</th>\n",
       "      <td>0344</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>1906</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.754717</td>\n",
       "      <td>183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>957</th>\n",
       "      <td>2175</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.740741</td>\n",
       "      <td>957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>963</th>\n",
       "      <td>2887</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.779661</td>\n",
       "      <td>963</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    patient  sleep   quality  epoch\n",
       "382    2896    1.0  0.666667    382\n",
       "633    0344    1.0  0.766667    633\n",
       "183    1906    2.0  0.754717    183\n",
       "957    2175    3.0  0.740741    957\n",
       "963    2887    2.0  0.779661    963"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "class_weights = class_weight.compute_class_weight('balanced', classes=sorted(np.unique(train_dataframe.sleep)), \n",
    "                                                  y=train_dataframe.sleep)\n",
    "class_weights = {i : class_weights[i]/min(class_weights) for i in range(len(class_weights))}\n",
    "\n",
    "train_dataframe_balanced = []\n",
    "for key, value in class_weights.items():\n",
    "    this_dataframe = train_dataframe[train_dataframe.sleep == key]\n",
    "    k = int(len(this_dataframe)*(value - 1))\n",
    "    new_df = this_dataframe.sample(n=k, replace=True)\n",
    "    train_dataframe_balanced.append(new_df)\n",
    "    \n",
    "train_dataframe_balanced = pd.concat(train_dataframe_balanced)\n",
    "train_dataframe_balanced = pd.concat([train_dataframe, train_dataframe_balanced])\n",
    "\n",
    "train_dataframe=train_dataframe_balanced\n",
    "\n",
    "train_dataframe = shuffle(train_dataframe)\n",
    "\n",
    "display(train_dataframe.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------Data Summary------\n",
      "Train Patients: 558\n",
      "Train Samples: 716406 have shape of 2400\n",
      "Train Sleep Stage Breakdown:\n",
      "1.0    179102\n",
      "0.0    179102\n",
      "3.0    179101\n",
      "2.0    179101\n",
      "Name: sleep, dtype: int64\n",
      "Validation Patients: 114\n",
      "Validation Samples: 72596 have shape of 2400\n",
      "Validation Sleep Stage Breakdown:\n",
      "1.0    41613\n",
      "0.0    13310\n",
      "3.0     9743\n",
      "2.0     7930\n",
      "Name: sleep, dtype: int64\n",
      "Test Patients: 81\n",
      "Test Samples: (51104, 4)\n",
      "Test Sleep Stage Breakdown:\n",
      "1.0    29724\n",
      "0.0     9019\n",
      "3.0     6971\n",
      "2.0     5390\n",
      "Name: sleep, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"------Data Summary------\")\n",
    "print(f'Train Patients: {len(train_ppg.keys())}')\n",
    "print(f'Train Samples: {train_dataframe.shape[0]} have shape of {list(train_ppg_subs.values())[0].shape[1]}')\n",
    "print(f'Train Sleep Stage Breakdown:\\n{train_dataframe[\"sleep\"].value_counts()}')\n",
    "\n",
    "print(f'Validation Patients: {len(validation_ppg.keys())}')\n",
    "print(f'Validation Samples: {validation_dataframe.shape[0]} have shape of {list(validation_ppg_subs.values())[0].shape[1]}')\n",
    "print(f'Validation Sleep Stage Breakdown:\\n{validation_dataframe[\"sleep\"].value_counts()}')\n",
    "\n",
    "print(f'Test Patients: {len(test_ppg.keys())}')\n",
    "print(f'Test Samples: {test_dataframe.shape}')\n",
    "print(f'Test Sleep Stage Breakdown:\\n{test_dataframe[\"sleep\"].value_counts()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, ppgs, sleep, dataframe, sleep_stage_mappings=None, batch_size=32, shuffle=True):\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.dataframe = dataframe\n",
    "        self.count = dataframe.shape[0]\n",
    "        \n",
    "        self.ppgs = ppgs\n",
    "        self.sleep= sleep\n",
    "        self.dim = None\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        batches =  int(np.ceil(self.count / self.batch_size))\n",
    "#         print(f'Samples: {self.count}. Batch Size: {self.batch_size}. Num_Batches: {batches})\n",
    "        return batches\n",
    "              \n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        \n",
    "        start = index*self.batch_size\n",
    "        end = (index+1)*self.batch_size\n",
    "        if end > self.count:\n",
    "            end = self.count\n",
    "            \n",
    "        indexes = self.indexes[start:end]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(indexes)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(self.count)\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, indexes):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        \n",
    "        # Initialization\n",
    "        X = np.empty((len(indexes), 2400))             # 4800 for 32 Hz\n",
    "        y = np.empty(len(indexes), dtype=int)\n",
    "\n",
    "        # Generate data\n",
    "        for i, indx in enumerate(indexes):\n",
    "            \n",
    "            patient = self.dataframe.iloc[indx].patient\n",
    "            data_index = self.dataframe.iloc[indx].epoch\n",
    "            sleep = self.dataframe.iloc[indx].sleep\n",
    "            \n",
    "            # Store sample\n",
    "            X[i] = self.ppgs[patient][data_index,:]\n",
    "\n",
    "            # Store class\n",
    "            y[i] = sleep.reshape(1,1) #self.sleep[patient][data_index,:][2].reshape(1, 1)\n",
    "                \n",
    "        \n",
    "        return X.reshape((X.shape[0], X.shape[1], 1)),  y.reshape((y.shape[0], 1))       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "import time\n",
    "@use_named_args(dimensions=dimensions)\n",
    "def run_fold(dim_single_fold, dim_epochs, dim_dropout, dim_learning_rate, dim_batch_size, dim_kernel_size, dim_channels):\n",
    "    t = time.time()\n",
    "    try:\n",
    "        print(f'Single Fold : {dim_single_fold}', f\"dim_epochs={dim_epochs}\", \n",
    "              f\"dim_dropout={dim_dropout}\",       f\"dim_learning_rate={dim_learning_rate}\",\n",
    "              f\"dim_batch_size={dim_batch_size}\", f\"dim_kernel_size={dim_kernel_size}\", \n",
    "              f\"dim_channels={dim_channels}\")\n",
    "\n",
    "        # Delete the Keras model with these hyper-parameters from memory.\n",
    "        try: \n",
    "            K.clear_session()\n",
    "            tf.compat.v1.reset_default_graph()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        dg_train = DataGenerator(train_ppg_subs, train_sleep_subs, train_dataframe, sleep_encoding, batch_size=np.power(2, dim_batch_size), shuffle=True)\n",
    "        dg_validate = DataGenerator(validation_ppg_subs, validation_sleep_subs, validation_dataframe, sleep_encoding, batch_size=np.power(2, dim_batch_size), shuffle=False)\n",
    "\n",
    "        model = SimpleSleepStageCNN(in_size= dT*windows*new_fs_PPG, \n",
    "                              out_size=SLEEP_STAGES, \n",
    "                              learning_rate=dim_learning_rate, \n",
    "                              dropout=dim_dropout, \n",
    "                              kernel_size=dim_kernel_size, \n",
    "                              channels=dim_channels)\n",
    "\n",
    "        print(model.summary())\n",
    "        print('Training model...')\n",
    "        train_validate_results = model.fit(dg_train,\n",
    "                                           validation_data=dg_validate,\n",
    "                                            use_multiprocessing=True,\n",
    "                                            workers = 8,\n",
    "                                            epochs = dim_epochs)\n",
    "\n",
    "\n",
    "        print('Calculating validation predictions...')\n",
    "        val_preds = model.predict(dg_validate, verbose=1, workers=8, use_multiprocessing=True)\n",
    "        val_true =  validation_dataframe.sleep\n",
    "\n",
    "        # Evaluation metrics\n",
    "        confusion = confusion_matrix(np.argmax(val_preds, axis=1), val_true)\n",
    "        confusion_norm = confusion / confusion.sum(axis=1).reshape(-1,1)\n",
    "        confusion_diag = np.diag(confusion_norm)\n",
    "        evaluation_metric = (np.median(confusion_diag) + 2 * np.min(confusion_diag))/3\n",
    "        \n",
    "        if np.isnan(evaluation_metric):\n",
    "            evaluation_metric = 0\n",
    "\n",
    "        print(\"Fold run time: \", time.time() - t)\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(confusion)\n",
    "        print(\"Normalized Confusion Matrix:\")\n",
    "        print(confusion_norm)\n",
    "        print(\"Evaluation Metric: diagonal median\")\n",
    "        print(evaluation_metric)\n",
    "\n",
    "#         return -evaluation_metric, model\n",
    "        return -evaluation_metric\n",
    "\n",
    "    except:\n",
    "        print(\"Could not run model with these params...\")\n",
    "        traceback.print_exc()\n",
    "#         return 9, None\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import logging\n",
    "# logging.getLogger('tensorflow').disabled = True\n",
    "\n",
    "\n",
    "# dim_single_fold = 'true'\n",
    "# dim_epochs = 5 \n",
    "# dim_dropout = 0.4\n",
    "# dim_learning_rate = 0.01\n",
    "# dim_batch_size = 8\n",
    "# dim_kernel_size = 5\n",
    "# dim_channels = 6\n",
    "# results = run_fold([dim_single_fold, dim_epochs,dim_dropout, dim_learning_rate, dim_batch_size, dim_kernel_size, dim_channels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Categorical(categories=('true',), prior=None), Integer(low=5, high=15, prior='uniform', transform='identity'), Real(low=0.0001, high=0.5, prior='uniform', transform='identity'), Real(low=0.0001, high=0.1, prior='uniform', transform='identity'), Integer(low=7, high=9, prior='uniform', transform='identity'), Integer(low=3, high=13, prior='uniform', transform='identity'), Integer(low=4, high=7, prior='uniform', transform='identity')]\n",
      "Single Fold : true dim_epochs=5 dim_dropout=0.25 dim_learning_rate=0.001 dim_batch_size=7 dim_kernel_size=8 dim_channels=5\n",
      "I am here\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 800, 32)           352       \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 800, 32)           8224      \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 400, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 400, 64)           16448     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 200, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 200, 128)          65664     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 100, 128)          0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                650       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 44        \n",
      "=================================================================\n",
      "Total params: 99,638\n",
      "Trainable params: 99,638\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Training model...\n",
      "Epoch 1/5\n",
      "5597/5597 [==============================] - 68s 12ms/step - loss: 1.1017 - accuracy: 0.2822 - val_loss: 1.1253 - val_accuracy: 0.2169\n",
      "Epoch 2/5\n",
      "5597/5597 [==============================] - 68s 12ms/step - loss: 0.8756 - accuracy: 0.2575 - val_loss: 1.1595 - val_accuracy: 0.3366\n",
      "Epoch 3/5\n",
      "5597/5597 [==============================] - 68s 12ms/step - loss: 0.7501 - accuracy: 0.2517 - val_loss: 1.1923 - val_accuracy: 0.2384\n",
      "Epoch 4/5\n",
      "5597/5597 [==============================] - 68s 12ms/step - loss: 0.6656 - accuracy: 0.2490 - val_loss: 1.2070 - val_accuracy: 0.2428\n",
      "Epoch 5/5\n",
      "3066/5597 [===============>..............] - ETA: 27s - loss: 0.6146 - accuracy: 0.2485"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.getLogger('tensorflow').disabled = True\n",
    "\n",
    "\n",
    "from skopt import gbrt_minimize, gp_minimize\n",
    "from utils.consts import *\n",
    "import time\n",
    "print(dimensions)\n",
    "# try:\n",
    "t = time.time()\n",
    "gp_result = gp_minimize(func=run_fold,\n",
    "                        dimensions=dimensions,\n",
    "                        n_calls=24,\n",
    "                        noise= 0.01,\n",
    "                        n_jobs=-1,\n",
    "                        kappa = 5,\n",
    "                        x0=default_parameters)  # change n_calls to how much you want.\n",
    "print(\"----------time elapsed----------\")\n",
    "print(t - time.time())\n",
    "# except ValueError:\n",
    "#     print(\"Oops!  That was no valid number.  Try again...\")\n",
    "    \n",
    "\n",
    "display(gp_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt.plots import plot_objective\n",
    "from utils.consts import *\n",
    "import time\n",
    "\n",
    "\n",
    "print(gp_result.x)\n",
    "df = pd.DataFrame(gp_result.x_iters, columns=dimensions_names)\n",
    "df[\"result\"] = gp_result.func_vals\n",
    "print(np.argmin(gp_result.func_vals))\n",
    "\n",
    "df.to_pickle('results/simple0_res' + str(time.time()) + '.pkl')\n",
    "df.style.apply(lambda x: ['background: lightgreen' if x.name == np.argmin(gp_result.func_vals)\n",
    "                              else '' for i in x], \n",
    "                   axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "\n",
    "\n",
    "def train_and_eval(dim_single_fold, dim_epochs,dim_dropout,dim_optimizer, dim_learning_rate, dim_activation,dim_batch_size, dim_kernel_size, dim_pool_size):\n",
    "    try:\n",
    "        print(f'Single Fold : {dim_single_fold}', \n",
    "                  f\"{dim_epochs=}\", \n",
    "                  f\"{dim_dropout=}\",\n",
    "                  f\"{dim_learning_rate=}\",\n",
    "                  f\"{dim_batch_size=}\", \n",
    "                  f\"{dim_kernel_size=}\", \n",
    "                  f\"{dim_channels=}\", \n",
    "\n",
    "\n",
    "        class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(train_dataframe.sleep), \n",
    "                                                          y=train_dataframe.sleep)\n",
    "        class_weights = {i : class_weights[i] for i in range(len(class_weights))}\n",
    "        print(\"Weights: \",  class_weights)\n",
    "        history = []\n",
    "        confusion = []\n",
    "        preds = []\n",
    "        trues = []\n",
    "\n",
    "        \n",
    "        \n",
    "        # Delete the Keras model with these hyper-parameters from memory.\n",
    "        try: \n",
    "            del model\n",
    "            K.clear_session()\n",
    "            tf.compat.v1.reset_default_graph()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        assert(len(intersection(train_dataframe.patient.unique(), test_dataframe.patient.unique())))==0\n",
    "        dg_train = DataGenerator(train_ppg_subs, train_sleep_subs, train_dataframe, sleep_encoding, batch_size=np.power(2, dim_batch_size), shuffle=True)\n",
    "        dg_test = DataGenerator(test_ppg_subs, test_sleep_subs, test_dataframe, sleep_encoding, batch_size=np.power(2, dim_batch_size), shuffle=False)\n",
    "\n",
    "        #TODO Remember we may need to apply class weights to the test set predictions!!!\n",
    "        model = SleepStageCNN(in_size= dT*windows*new_fs_PPG, \n",
    "                              out_size=SLEEP_STAGES, \n",
    "                              learning_rate=dim_learning_rate, \n",
    "                              dropout=dim_dropout, \n",
    "                              kernel_size=dim_kernel_size, \n",
    "                              channels=dim_channels)\n",
    "\n",
    "        print(model.summary())\n",
    "        print('Training model...')\n",
    "        train_test_results = model.fit(dg_train,\n",
    "                                            validation_data=dg_test,\n",
    "                                            class_weight=class_weights,\n",
    "                                            use_multiprocessing=True,\n",
    "                                            workers = 8,\n",
    "                                            epochs = dim_epochs)\n",
    "\n",
    "\n",
    "        print('Calculating test predictions...')\n",
    "        test_preds = model.predict(dg_test, verbose=1, workers=8, use_multiprocessing=True)\n",
    "        test_true =  test_dataframe.sleep\n",
    "\n",
    "        preds.append(test_preds)\n",
    "        trues.append(test_true)\n",
    "        history.append(train_test_results)      \n",
    "\n",
    "        # Evaluation metrics\n",
    "        confusion = []\n",
    "        for i in range(1):\n",
    "            confusion.append(confusion_matrix(np.argmax(preds[i], axis=1), trues[i]))\n",
    "\n",
    "        confusion = np.array(confusion).sum(axis=0)\n",
    "        confusion_norm = confusion / confusion.sum(axis=1).reshape(-1,1)\n",
    "        confusion_diag = np.diag(confusion_norm)\n",
    "        evaluation_metric = np.median(confusion_diag) + 2 * np.min(confusion_diag)\n",
    "        \n",
    "        if np.isnan(evaluation_metric):\n",
    "            evaluation_metric = 0\n",
    "\n",
    "        print(\"Test Confusion Matrix:\")\n",
    "        print(confusion)\n",
    "        print(\"Test Normalized Confusion Matrix:\")\n",
    "        print(confusion_norm)\n",
    "        print(\"Test Evaluation Metric: diagonal median\")\n",
    "        print(evaluation_metric)\n",
    "\n",
    "        return -evaluation_metric\n",
    "\n",
    "    except:\n",
    "        print(\"Could not run model with these params...\")\n",
    "        traceback.print_exc()\n",
    "#         return 9, None\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_results = train_and_eval(*gp_result.x)\n",
    "# test_results = train_and_eval(dim_single_fold = 'true',dim_epochs = 20 ,dim_dropout = 0.25,dim_optimizer = 'adam' ,dim_learning_rate = 0.001,dim_activation = 'relu',dim_batch_size = 5,dim_kernel_size = 3,dim_pool_size = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patient = list(train_ppg.keys())[5]\n",
    "# t_sleep = np.arange(0, len(train_sleep[patient]),1)\n",
    "# t_ppg = np.arange(0, len(train_ppg[patient])/(32*30), 1/(32*30))\n",
    "# t_quality = np.arange(0, len(train_quality[patient]), 1)          \n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(10,5))\n",
    "# # # plt.plot(t_ppg, train_ppg[patient])\n",
    "# # plt.plot(t_sleep, train_sleep[patient])\n",
    "# # plt.plot(t_quality, train_quality[patient])\n",
    "# train_sleep[patient][train_sleep[patient] > 4] = 0\n",
    "\n",
    "# color = 'tab:red'\n",
    "# ax.plot(t_sleep, train_sleep[patient], label='Sleep Stage')\n",
    "# ax.plot(t_quality, train_quality[patient], label='Quality')\n",
    "# ax.set_ylabel('Value', color=color)\n",
    "# ax.set_xlabel('Time [N Epoch of 30s]', color=color)\n",
    "# ax.legend(prop={'size': 10})\n",
    "\n",
    "\n",
    "# fig, ax1 = plt.subplots(figsize=(20,20))\n",
    "\n",
    "# color = 'tab:red'\n",
    "# ax1.set_xlabel('time (s)')\n",
    "# ax1.set_ylabel('Sleep Stage', color=color)\n",
    "# ax1.plot(t_sleep, train_sleep[patient], color=color)\n",
    "# ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "# ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "# color = 'tab:blue'\n",
    "# ax2.set_ylabel('Signal Quality', color=color)  # we already handled the x-label with ax1\n",
    "# ax2.plot(t_quality, train_quality[patient], color=color)\n",
    "# ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "# fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
